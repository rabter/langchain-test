import os
from dotenv import load_dotenv
from langchain_community.vectorstores import vectara
from langchain_core.prompts import PromptTemplate
from langchain_core.tools import retriever
from langchain_openai import ChatOpenAI
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from pydantic import SecretStr
from functools import lru_cache
import time
import unicodedata
import sys

# RAG íŒë³„ ê¸°ì¤€ ìƒìˆ˜: ì´ë³´ë‹¤ í¬ë©´ ìœ ì‚¬í•˜ì§€ ì•Šë‹¤ê³  ê°„ì£¼
MAX_DISTANCE_FOR_RAG = 1.1 # ChromaëŠ” ê±°ë¦¬ ê¸°ë°˜ì´ë¯€ë¡œ, 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬, ë„ˆë¬´ ë©€ë©´ ê·¸ëƒ¥ fallback(ì˜ˆì™¸ ë°©ì§€ìš© ìµœëŒ€ê°’)
SCORE_GAP_THRESHOLD = 0.25   # ë¬¸ì„œ 1~2ìœ„ ê±°ë¦¬ ì°¨ì´ê°€ ìž‘ìœ¼ë©´ ìœ ì‚¬í•œ ë¬¸ì„œë¡œ íŒë‹¨

def normalize_text(text: str) -> str:
    """
    ì‚¬ìš©ìž ìž…ë ¥ ì¿¼ë¦¬ì—ì„œ ìœ ë‹ˆì½”ë“œ ì •ê·œí™” ë° ê¹¨ì§„ ë¬¸ìž ì œê±°
    - NFC ì •ê·œí™”: ìœ ë‹ˆì½”ë“œ ì¡°í•©í˜• ë¬¸ì œ í•´ê²°
    - UTF-8 ë””ì½”ë”© ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ surrogate ë¬¸ìž ì œê±°
    """

    # ìœ ë‹ˆì½”ë“œ ì •ê·œí™” (NFC: ê¶Œìž¥ë°©ì‹)
    text = unicodedata.normalize("NFC", text)

    # ê¹¨ì§„ ë¬¸ìž(ìž˜ë¦° ìœ ë‹ˆì½”ë“œ ë“±) ì œê±°
    return text.encode("utf-8", errors="ignore").decode("utf-8")

def safe_input(prompt: str = "") -> str:
    sys.stdout.write(prompt)
    sys.stdout.flush()
    try:
        line = sys.stdin.buffer.readline()
        return line.decode("utf-8", errors="ignore").strip()
    except Exception as e:
        print(f"ìž…ë ¥ ì˜¤ë¥˜: {e}")
        return ""
# .env íŒŒì¼ ë¡œë“œ
start = time.time()
_ = load_dotenv()
print(f"âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {time.time() - start:.4f}ì´ˆ")

@lru_cache(maxsize=1)
def get_embedding():
    print("ðŸ”„ ìž„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘... (ìµœì´ˆ 1íšŒ)")
    return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

start = time.time()
embedding = get_embedding()

# embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = Chroma(persist_directory="../rag_db", embedding_function=embedding)
print(f"ðŸ”Ž ë²¡í„° DB ë‚´ ë¬¸ì„œ ìˆ˜: {vectorstore._collection.count()}ê°œ")


# Groq API ê¸°ë°˜ LLM
llm = ChatOpenAI(
    model="gemma2-9b-it",
    temperature=0,
        api_key=SecretStr(os.getenv("OPENAI_API_KEY") or "") if os.getenv("OPENAI_API_KEY") else None,
        base_url=os.getenv("OPENAI_API_BASE")
)


# RAG ì²´ì¸ ìƒì„±
# ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ë©´ì„œ ìž„ê³„ê°’ ì¡°ê±´ë„ ì¶”ê°€í•˜ëŠ” ë°©ì‹(similarity_score_threshold)
retriever = vectorstore.as_retriever()    


qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)

# âœ… ì¼ë°˜ ì§€ì‹ ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def answer_with_llm_only(llm, question: str) -> str:
    prompt = PromptTemplate.from_template("ë‹¤ìŒ ì§ˆë¬¸ì— ì¼ë°˜ ì§€ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ì¤˜:\n\n{question}")

    return llm.invoke(prompt.format(question=question)).content

# âœ… RAG ì—¬ë¶€ íŒë‹¨ + ë¶„ê¸° ì²˜ë¦¬
def get_rag_answer(query: str) -> str:
    print(f"[DEBUG] query value: {query}")

    if not isinstance(query, str):
        raise ValueError("ì¿¼ë¦¬ëŠ” Stringì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
    
    # ìœ ì‚¬ë„ + ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    try:
        docs_with_score = vectorstore.similarity_search_with_score(query, k=3)
        for i, (doc, score) in enumerate(docs_with_score, 1):
            print(f"[#{i}] ê±°ë¦¬ ì ìˆ˜: {score:.4f}")
            print(doc.page_content[:200])
            print("-" * 40)        
    except Exception as e:
        print(f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return answer_with_llm_only(llm, query)

    # ë¬¸ì„œê°€ ì•„ì˜ˆ ì—†ì„ ê²½ìš°
    if not docs_with_score:
        print("âŒ ë¬¸ì„œê°€ ì•„ì˜ˆ ê²€ìƒ‰ë˜ì§€ ì•ŠìŒ")
        return answer_with_llm_only(llm, query)
    
    # ê±°ë¦¬ ì ìˆ˜ ì¶”ì¶œ ë° ë¡œê·¸
    scores = [score for _, score in docs_with_score]

    # ê°€ìž¥ ìœ ì‚¬í•œ ë¬¸ì„œì˜ ìœ ì‚¬ë„ ì ìˆ˜ í™•ì¸
    top_doc, top_score = docs_with_score[0]

    print(f"\n ìœ ì‚¬ë„ ê±°ë¦¬ ì ìˆ˜:")
    for i, score in enumerate(scores, 1):
        print(f" - #{i} ë¬¸ì„œ ê±°ë¦¬ ì ìˆ˜: {score:.4f}")
    print(f"âž¡ ìµœê³  ê±°ë¦¬ ì ìˆ˜: {top_score:.4f}")

    score_gap = scores[1] - scores[0] if len(scores) > 1 else 1.0
    is_relevant = (top_score < MAX_DISTANCE_FOR_RAG and score_gap < SCORE_GAP_THRESHOLD)

    print(f"ðŸ“ ê±°ë¦¬ ê¸°ì¤€ íŒë‹¨ â†’ top_score < {MAX_DISTANCE_FOR_RAG} = {top_score < MAX_DISTANCE_FOR_RAG}")
    print(f"ðŸ“ ê±°ë¦¬ ì°¨ íŒë‹¨ â†’ gap < {SCORE_GAP_THRESHOLD} = {score_gap < SCORE_GAP_THRESHOLD}")
    print(f"ðŸ“Œ ìµœì¢… ìœ ì‚¬ ì—¬ë¶€ íŒë‹¨ ê²°ê³¼: {is_relevant}")

    # ì‘ë‹µ ë¶„ê¸°
    if is_relevant:
        print("âœ… ê´€ë ¨ ë¬¸ì„œê°€ ì¶©ë¶„ížˆ ìœ ì‚¬í•˜ì—¬ RAG ê¸°ë°˜ ì‘ë‹µ ìˆ˜í–‰")
        return qa_chain.invoke({"query": query})["result"]
    else:
        print("ðŸŒ ê´€ë ¨ ë¬¸ì„œì˜ ìœ ì‚¬ë„ê°€ ë‚®ì•„ ì¼ë°˜ ì§€ì‹ ê¸°ë°˜ ì‘ë‹µìœ¼ë¡œ fallback")
        return answer_with_llm_only(llm, query)
