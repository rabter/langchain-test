import os
import time
from functools import lru_cache
from dotenv import load_dotenv
from pydantic import SecretStr
from langchain_core.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI

# .env í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë²¡í„° ê±°ë¦¬ ê¸°ì¤€ ì„¤ì •
MAX_DISTANCE_FOR_RAG = 2.0
SCORE_GAP_THRESHOLD = 0.5

@lru_cache(maxsize=1)
def get_embedding():
    print("ðŸ”„ ìž„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
    return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

embedding = get_embedding()

# ë²¡í„°ìŠ¤í† ì–´ ì—°ê²°
vectorstore = Chroma(persist_directory="../report.db", embedding_function=embedding)
print(f"ðŸ“¦ Report ë²¡í„° DB ë¬¸ì„œ ìˆ˜: {vectorstore._collection.count()}ê°œ")

# Groq ê¸°ë°˜ LLM ì •ì˜
llm = ChatOpenAI(
    model="gemma2-9b-it",
    temperature=0.3,
    api_key=SecretStr(os.getenv("OPENAI_API_KEY") or "") if os.getenv("OPENAI_API_KEY") else None,
    base_url=os.getenv("OPENAI_API_BASE")
)

# ë¦¬í¬íŒ… ì „ìš© í”„ë¡¬í”„íŠ¸
custom_prompt = PromptTemplate.from_template(
    """
    ë‹¤ìŒì€ í´ë¼ìš°ë“œ ìžì›ì˜ ì‚¬ìš©ëŸ‰ ë¦¬í¬íŠ¸ìž…ë‹ˆë‹¤:

    {context}

    ìœ„ ë¦¬í¬íŠ¸ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•„ëž˜ ì§ˆë¬¸ì— ëŒ€í•´ **ì •í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ** í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
    - ì œëª©ì„ ë¶™ì´ê³ , ë§ˆí¬ë‹¤ìš´ í—¤ë” (`##`) í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.
    - ì‚¬ìš©ëŸ‰ ë³€í™”(%)ë¥¼ ìš”ì•½í•˜ê³ , ìžì›ë³„ë¡œ í•­ëª©í™”í•˜ì„¸ìš”.
    - ê°€ìž¥ ë§Žì´ ì‚¬ìš©ëœ ìžì›ì„ ê°•ì¡°í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”.
    - ì‚¬ìš©ëŸ‰ ìˆ˜ì¹˜ë¥¼ ê°€ëŠ¥í•œ í•œ ìžì—°ì–´ë¡œ ìš”ì•½í•˜ì„¸ìš”.
    - CPU, ë©”ëª¨ë¦¬, ë””ìŠ¤í¬ ì¤‘ ì–´ë–¤ ìžì›ì´ ë§Žì´ ì‚¬ìš©ë˜ê³  ìžˆëŠ”ì§€ ë¶„ì„í•´ ì£¼ì„¸ìš”.
    - ìˆ«ìžë§Œ ë‚˜ì—´í•˜ì§€ ë§ê³ , ë¹„ìœ¨ì˜ ë†’ê³  ë‚®ìŒì„ ê¸°ì¤€ìœ¼ë¡œ ìƒíƒœë¥¼ íŒë‹¨í•´ ì£¼ì„¸ìš”.
    - ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ì •ë³´ëŠ” ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.
    - **ë¬¸ì„œì— ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ì •ë³´ëŠ” ì ˆëŒ€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.**

    ì§ˆë¬¸: {question}

    ë‹µë³€:
    """
)

retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type_kwargs={"prompt": custom_prompt},
    return_source_documents=True,
    verbose=True
)

def answer_with_llm_only(llm, question: str) -> str:
    fallback_prompt = PromptTemplate.from_template("ë‹¤ìŒ ì§ˆë¬¸ì— ì¼ë°˜ ì§€ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ì¤˜:\n\n{question}")
    return llm.invoke(fallback_prompt.format(question=question)).content

def get_rag_answer(query: str) -> str:
    if not isinstance(query, str):
        raise ValueError("ì¿¼ë¦¬ëŠ” ë¬¸ìžì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    try:
        # contextë¥¼ ì œëŒ€ë¡œ ê°€ì ¸ì˜¤ëŠ”ì§€ í™•ì¸
        docs = vectorstore.similarity_search(query, k=5)
        print(f"ðŸ“„ RAG ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")
        for i, doc in enumerate(docs):
            print(f"[{i}] {doc.page_content[:200]}...")  # ì²« 200ìž ì •ë„


        docs_with_score = vectorstore.similarity_search_with_score(query, k=3)
        print(f"docs_with_score: {docs_with_score}")

    except Exception as e:
        print(f"ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return answer_with_llm_only(llm, query)

    if not docs_with_score:
        return answer_with_llm_only(llm, query)

    scores = [score for _, score in docs_with_score]
    top_doc, top_score = docs_with_score[0]
    score_gap = scores[1] - scores[0] if len(scores) > 1 else 1.0

    print(f"top_score: {top_score}, score_gap: {score_gap}")


    is_relevant = (top_score < MAX_DISTANCE_FOR_RAG and score_gap < SCORE_GAP_THRESHOLD)

    if is_relevant:
        retrieved_docs = retriever.get_relevant_documents(query)  # Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
        # ê° ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ë§Œ ë½‘ì•„ì„œ "\n\n"ìœ¼ë¡œ ì—°ê²°
        retrieved_docs_text = "\n\n".join(doc.page_content for doc in retrieved_docs)

        formatted_prompt = custom_prompt.format(context=retrieved_docs_text, question=query)
        print("---- LLMì—ê²Œ ì „ë‹¬ë˜ëŠ” í”„ë¡¬í”„íŠ¸ ----")
        print(formatted_prompt)
        print("----------------------------")
        return qa_chain.invoke({"query": query})["result"]
    else:
        return answer_with_llm_only(llm, query)
