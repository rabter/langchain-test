import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
# from langchain_community.embeddings import HuggingFaceEmbeddings (deprecated)
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from pydantic import SecretStr

# .env íŒŒì¼ ë¡œë“œ
_ = load_dotenv()

# ë¬¸ì„œ ë¡œë“œ
loader = TextLoader("rag_sample.txt", encoding="utf-8")
documents = loader.load()

# ë¬¸ì„œ ìª¼ê°œê¸°
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
splits = splitter.split_documents(documents)

# í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (ë¡œì»¬ HuggingFace ëª¨ë¸ ì‚¬ìš©)
embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# ë²¡í„° DB ìƒì„±
vectorstore = Chroma.from_documents(splits, embedding)

# Groq API ê¸°ë°˜ LLM
llm = ChatOpenAI(
    model="gemma2-9b-it",
    temperature=0,
        api_key=SecretStr(os.getenv("OPENAI_API_KEY") or "") if os.getenv("OPENAI_API_KEY") else None,
        base_url=os.getenv("OPENAI_API_BASE")
)

# RAG ì²´ì¸ ìƒì„±
qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectorstore.as_retriever())

# ì§ˆë¬¸ ì‹¤í–‰
# question = "SPEROì˜ êµ¬ì¡°ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
# question = "SPEROì˜ êµ¬ì¡°ì—ì„œ Backendë¥¼ êµ¬ì„±í•˜ëŠ” ìš”ì†Œë“¤ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
question = "SPEROì˜ êµ¬ì¡°ì—ì„œ Frontendì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."
response = qa_chain.invoke(question)

print("ğŸ’¬ ì‘ë‹µ(ì§ˆë¬¸):\n", response.get("query"))
print("ğŸ’¬ ì‘ë‹µ(ë‹µë³€):\n", response.get("result"))