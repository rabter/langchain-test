import os
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langchain_core.prompts import ChatPromptTemplate
from pydantic import SecretStr
from dotenv import load_dotenv
from typing import cast

# .env íŒŒì¼ ë¡œë”©
_ = load_dotenv()

# LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
llm = ChatOpenAI(
        # model="llama3-8b-8192",
        model="gemma2-9b-it",
        temperature=0.7,
        api_key=SecretStr(os.getenv("OPENAI_API_KEY") or "") if os.getenv("OPENAI_API_KEY") else None,
        base_url=os.getenv("OPENAI_API_BASE")
)

# 1. ê¸°ë³¸ ë©”ì‹œì§€ ì „ì†¡ ë° ì‘ë‹µ ì¶œë ¥
# response = llm.invoke([
#     HumanMessage(content="LangChainì€ ì–´ë–¤ ë„êµ¬ì´ê³ , ì–´ë–¤ ìš©ë„ë¡œ ì“°ì´ë‚˜ìš”?")
# ])

# print("ğŸ’¬ ì‘ë‹µ:\n", cast(str, response.content))

# 2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì˜ˆì œ
prompt = ChatPromptTemplate.from_template(
    "ë„ˆëŠ” ì „ë¬¸ì ì¸ ê¸°ìˆ  ì„¤ëª…ê°€ì•¼. ë‹¤ìŒ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ìì„¸íˆ ë‹µë³€í•´ì¤˜: \n\nì§ˆë¬¸: {question}"
)

prompt_template = prompt | llm

response = prompt_template.invoke({"question": "RAGëŠ” ë¬´ì—‡ì¸ê°€ìš”?"})
print("ğŸ’¬ ì‘ë‹µ:\n", response.content)
